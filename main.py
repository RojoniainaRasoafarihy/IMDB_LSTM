# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q5XMTptWjC6h6X0dXho3Hnm3EHi5TDDx
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_files
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import LabelEncoder
import numpy as np
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from torchtext.vocab import GloVe
from model import LSTMModel


class IMDBDataset(Dataset):


    def __init__(self, data, labels):
        self.data = torch.tensor(data, dtype=torch.long)
        self.labels = torch.tensor(labels, dtype=torch.float32)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        return self.data[index], self.labels[index]


def preprocess_data(texts, vocab, max_len):

    stop_words = set(stopwords.words("english"))
    sequences = []

    for text in texts:
        tokens = word_tokenize(text.lower())
        tokens = [word for word in tokens if word.isalnum() and word not in stop_words]
        encoded = [vocab.get(word, vocab["<unk>"]) for word in tokens]
        sequences.append(encoded[:max_len])

    # Padding
    padded_sequences = np.zeros((len(sequences), max_len), dtype=np.int64)
    for i, seq in enumerate(sequences):
        padded_sequences[i, :len(seq)] = seq

    return padded_sequences


def train_model(model, dataloader, criterion, optimizer, device):

    model.train()
    total_loss = 0

    for inputs, labels in dataloader:
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs).squeeze()
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    return total_loss / len(dataloader)


def evaluate_model(model, dataloader, device):

    model.eval()
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs).squeeze()
            predicted = (outputs >= 0.5).float()
            correct += (predicted == labels).sum().item()
            total += labels.size(0)

    return correct / total


def main():

    # Load IMDB data
    data = load_files(
        "aclImdb/train/",
        categories=["pos", "neg"],
        encoding="utf-8",
        shuffle=True
    )
    texts, labels = data.data, data.target

    # Label encoding
    label_encoder = LabelEncoder()
    labels = label_encoder.fit_transform(labels)

    # Build vocabulary
    vectorizer = CountVectorizer(max_features=10000, token_pattern=r'\b\w+\b')
    vectorizer.fit(texts)
    vocab = {word: i for i, word in enumerate(vectorizer.get_feature_names_out())}
    vocab["<unk>"] = len(vocab)

    # Preprocess data
    max_len = 300
    X = preprocess_data(texts, vocab, max_len)
    y = np.array(labels)

    # Train/validation split
    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # Create PyTorch datasets and dataloaders
    train_dataset = IMDBDataset(X_train, y_train)
    val_dataset = IMDBDataset(X_val, y_val)
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)

    # Initialize the LSTM model
    vocab_size = len(vocab)
    embedding_dim = 100
    hidden_dim = 128
    output_dim = 1
    model = LSTMModel(vocab_size, embedding_dim, hidden_dim, output_dim)

    # Load pre-trained embeddings (GloVe)
    glove = GloVe(name="6B", dim=embedding_dim)
    embeddings = np.random.uniform(-0.25, 0.25, (vocab_size, embedding_dim))
    for word, idx in vocab.items():
        if word in glove.stoi:
            embeddings[idx] = glove.vectors[glove.stoi[word]]
    model.embedding.weight.data.copy_(torch.tensor(embeddings))

    # Set device (CPU or GPU)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    # Define loss and optimizer
    criterion = nn.BCELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # Training and evaluation loop
    epochs = 5
    for epoch in range(epochs):
        train_loss = train_model(model, train_loader, criterion, optimizer, device)
        val_accuracy = evaluate_model(model, val_loader, device)

        print(
            f"Epoch {epoch + 1}/{epochs}, "
            f"Loss: {train_loss:.4f}, "
            f"Validation Accuracy: {val_accuracy:.4f}"
        )




if __name__ == "__main__":
    main()